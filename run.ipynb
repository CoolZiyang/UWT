{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize folder and logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Start time: 2018-05-12 14:09:12\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "assert torch.cuda.is_available(), \"torch.cuda is not available\"\n",
    "\n",
    "## create experiment folder\n",
    "log_dir = os.path.join(os.getcwd(),'log')\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "exp_dir = os.path.join(log_dir, datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "if not os.path.exists(exp_dir):\n",
    "    os.makedirs(exp_dir)\n",
    "\n",
    "## set up logger\n",
    "log_file = os.path.join(exp_dir, 'train.log')\n",
    "file_handler = logging.FileHandler(log_file)\n",
    "file_handler.setLevel(logging.DEBUG)\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.DEBUG)\n",
    "logger = logging.getLogger()\n",
    "logger.handlers = []\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.propagate = False\n",
    "logger.addHandler(file_handler)\n",
    "logger.addHandler(console_handler)\n",
    "logger.info('Start time: %s' % datetime.now().strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loaded 200000 pre-trained en word embeddings\n",
      "loaded 200000 pre-trained es word embeddings\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3075e-01, -8.7659e-02, -1.1427e-01,  ..., -4.0476e-02,\n",
       "         -1.2293e-02,  4.2569e-02],\n",
       "        [-3.6446e-01,  9.5962e-02, -1.6188e-01,  ..., -1.4986e-01,\n",
       "          2.3584e-01,  1.8541e-01],\n",
       "        [-5.9110e-02, -8.3343e-02, -9.3019e-02,  ..., -5.4064e-02,\n",
       "          1.7285e-01,  1.6713e-01],\n",
       "        ...,\n",
       "        [ 3.2125e-01,  1.3622e-01, -5.0101e-01,  ...,  1.4182e-01,\n",
       "          5.0989e-01,  2.2007e-01],\n",
       "        [-4.6783e-01, -7.4949e-01, -4.4708e-02,  ...,  9.5594e-01,\n",
       "         -3.6959e-01,  1.0554e-01],\n",
       "        [-7.4782e-02, -3.6216e-01, -1.8766e-01,  ..., -2.3346e-01,\n",
       "          6.2097e-02, -2.3693e-01]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import io\n",
    "from dict import Dictionary\n",
    "\n",
    "src_emb_file = './data/wiki.en.vec'\n",
    "tgt_emb_file = './data/wiki.es.vec'\n",
    "emb_dim = 300\n",
    "max_voc = 200000\n",
    "\n",
    "def load_embedding(file, tag):\n",
    "    word2id = {}\n",
    "    vectors = []\n",
    "    with io.open(file, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i == 0:\n",
    "                split = line.split()\n",
    "                assert len(split) == 2\n",
    "                assert emb_dim == int(split[1])\n",
    "            else:\n",
    "                if len(word2id) >= max_voc:\n",
    "                    break\n",
    "                word, vect = line.rstrip().split(' ', 1)\n",
    "                word = word.lower()\n",
    "                vect = np.fromstring(vect, sep=' ')\n",
    "                if np.linalg.norm(vect) == 0:\n",
    "                    vect[0] = 0.01\n",
    "                if word in word2id:\n",
    "                    logger.warning(\"word %s appears twice in the %s embedding\" % (word, 'source'))\n",
    "                    continue\n",
    "                else:\n",
    "                    if not vect.shape == (emb_dim,):\n",
    "                        logger.warning(\"invalid dimension (%i,) for %s word %s\" % (vect.shape[0], tag, word))\n",
    "                        continue\n",
    "                    word2id[word] = len(word2id)\n",
    "                    vectors.append(vect[None])\n",
    "    assert len(word2id) == len(vectors)\n",
    "    id2word = {v: k for k, v in word2id.items()}\n",
    "    dico = Dictionary(id2word, word2id, tag)\n",
    "    embeddings = np.concatenate(vectors, 0)\n",
    "    embeddings = torch.from_numpy(embeddings).float()\n",
    "    embeddings = embeddings.cuda()\n",
    "    logger.info(\"loaded %i pre-trained %s word embeddings\" % (len(vectors), tag))\n",
    "    \n",
    "    return dico, embeddings\n",
    "\n",
    "## load source embedding\n",
    "src_dico, _src_emb = load_embedding(src_emb_file, 'en')\n",
    "src_emb = torch.nn.Embedding(len(src_dico), emb_dim, sparse=True)\n",
    "src_emb.weight.data.copy_(_src_emb)\n",
    "\n",
    "## load target embedding\n",
    "tgt_dico, _tgt_emb = load_embedding(tgt_emb_file, 'es')\n",
    "tgt_emb = torch.nn.Embedding(len(tgt_dico), emb_dim, sparse=True)\n",
    "tgt_emb.weight.data.copy_(_tgt_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
