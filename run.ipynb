{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize folder and logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Start time: 2018-05-20 14:37:38\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "assert torch.cuda.is_available(), \"torch.cuda is not available\"\n",
    "\n",
    "## create experiment folder\n",
    "log_dir = os.path.join(os.getcwd(),'log')\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "exp_dir = os.path.join(log_dir, datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "if not os.path.exists(exp_dir):\n",
    "    os.makedirs(exp_dir)\n",
    "\n",
    "## set up logger\n",
    "log_file = os.path.join(exp_dir, 'train.log')\n",
    "file_handler = logging.FileHandler(log_file)\n",
    "file_handler.setLevel(logging.DEBUG)\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.DEBUG)\n",
    "logger = logging.getLogger()\n",
    "logger.handlers = []\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.propagate = False\n",
    "logger.addHandler(file_handler)\n",
    "logger.addHandler(console_handler)\n",
    "logger.info('Start time: %s' % datetime.now().strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===== Argument List =====\n",
      "Source Language: en\n",
      "Target Language: es\n",
      "Embedding Dimension: 300\n",
      "Vocabulary Size (for both): 200000\n",
      "Discriminator Hidden Layer Dimension: 2048\n",
      "Discriminator Hidden Dropout: 0.00\n",
      "Discriminator Input Dropout: 0.10\n",
      "Learning Rate: 0.10\n",
      "Decay: 0.95\n",
      "Number of Epochs: 50\n",
      "Number of Iterations per Epoch: 200000\n",
      "Batch Size: 200000\n",
      "Number of Steps for Discriminator: 200000\n",
      "Number of Most Frequent Words Fed into Discriminator: 75000\n",
      "Discriminator Smothiness: 0.1\n",
      "Orthogonality Update Coefficient: 0.001\n"
     ]
    }
   ],
   "source": [
    "src_emb_file = './data/wiki.en.vec'\n",
    "tgt_emb_file = './data/wiki.es.vec'\n",
    "src_lang = 'en'\n",
    "tgt_lang = 'es'\n",
    "emb_dim = 300\n",
    "max_voc = 200000\n",
    "\n",
    "dis_hid_dim = 2048\n",
    "dis_dropout = 0.\n",
    "dis_input_dropout = 0.1\n",
    "lr = 0.1\n",
    "decay = 0.95\n",
    "\n",
    "n_epochs = 50\n",
    "epoch_size = 200000\n",
    "batch_size = 32\n",
    "dis_steps = 5\n",
    "dis_most_frequent = 75000\n",
    "dis_smooth = 0.1\n",
    "map_beta = 0.001\n",
    "\n",
    "logger.info('===== Argument List =====')\n",
    "logger.info('Source Language: %s' % src_lang)\n",
    "logger.info('Target Language: %s' % tgt_lang)\n",
    "logger.info('Embedding Dimension: %i' % emb_dim)\n",
    "logger.info('Vocabulary Size (for both): %i' % max_voc)\n",
    "logger.info('Discriminator Hidden Layer Dimension: %i' % dis_hid_dim)\n",
    "logger.info('Discriminator Hidden Dropout: %.2f' % dis_dropout)\n",
    "logger.info('Discriminator Input Dropout: %.2f' % dis_input_dropout)\n",
    "logger.info('Learning Rate: %.2f' % lr)\n",
    "logger.info('Decay: %.2f' % decay)\n",
    "logger.info('Number of Epochs: %i' % n_epochs)\n",
    "logger.info('Number of Iterations per Epoch: %i' % epoch_size)\n",
    "logger.info('Batch Size: %i' % epoch_size)\n",
    "logger.info('Number of Steps for Discriminator: %i' % epoch_size)\n",
    "logger.info('Number of Most Frequent Words Fed into Discriminator: %i' % dis_most_frequent)\n",
    "logger.info('Discriminator Smothiness: %.1f' % dis_smooth)\n",
    "logger.info('Orthogonality Update Coefficient: %.3f' % map_beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loaded 200000 pre-trained en word embeddings\n",
      "loaded 200000 pre-trained es word embeddings\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3075e-01, -8.7659e-02, -1.1427e-01,  ..., -4.0476e-02,\n",
       "         -1.2293e-02,  4.2569e-02],\n",
       "        [-3.6446e-01,  9.5962e-02, -1.6188e-01,  ..., -1.4986e-01,\n",
       "          2.3584e-01,  1.8541e-01],\n",
       "        [-5.9110e-02, -8.3343e-02, -9.3019e-02,  ..., -5.4064e-02,\n",
       "          1.7285e-01,  1.6713e-01],\n",
       "        ...,\n",
       "        [ 3.2125e-01,  1.3622e-01, -5.0101e-01,  ...,  1.4182e-01,\n",
       "          5.0989e-01,  2.2007e-01],\n",
       "        [-4.6783e-01, -7.4949e-01, -4.4708e-02,  ...,  9.5594e-01,\n",
       "         -3.6959e-01,  1.0554e-01],\n",
       "        [-7.4782e-02, -3.6216e-01, -1.8766e-01,  ..., -2.3346e-01,\n",
       "          6.2097e-02, -2.3693e-01]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import io\n",
    "from dict import Dictionary\n",
    "\n",
    "def load_embedding(file, tag):\n",
    "    word2id = {}\n",
    "    vectors = []\n",
    "    with io.open(file, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i == 0:\n",
    "                split = line.split()\n",
    "                assert len(split) == 2\n",
    "                assert emb_dim == int(split[1])\n",
    "            else:\n",
    "                if len(word2id) >= max_voc:\n",
    "                    break\n",
    "                word, vect = line.rstrip().split(' ', 1)\n",
    "                word = word.lower()\n",
    "                vect = np.fromstring(vect, sep=' ')\n",
    "                if np.linalg.norm(vect) == 0:\n",
    "                    vect[0] = 0.01\n",
    "                if word in word2id:\n",
    "                    logger.warning(\"word %s appears twice in the %s embedding\" % (word, 'source'))\n",
    "                    continue\n",
    "                else:\n",
    "                    if not vect.shape == (emb_dim,):\n",
    "                        logger.warning(\"invalid dimension (%i,) for %s word %s\" % (vect.shape[0], tag, word))\n",
    "                        continue\n",
    "                    word2id[word] = len(word2id)\n",
    "                    vectors.append(vect[None])\n",
    "    assert len(word2id) == len(vectors)\n",
    "    id2word = {v: k for k, v in word2id.items()}\n",
    "    dico = Dictionary(id2word, word2id, tag)\n",
    "    embeddings = np.concatenate(vectors, 0)\n",
    "    embeddings = torch.from_numpy(embeddings).float()\n",
    "    embeddings = embeddings.cuda()\n",
    "    logger.info(\"loaded %i pre-trained %s word embeddings\" % (len(vectors), tag))\n",
    "    \n",
    "    return dico, embeddings\n",
    "\n",
    "## load source embedding\n",
    "src_dico, _src_emb = load_embedding(src_emb_file, src_lang)\n",
    "src_emb = torch.nn.Embedding(len(src_dico), emb_dim, sparse=True)\n",
    "src_emb.weight.data.copy_(_src_emb)\n",
    "\n",
    "## load target embedding\n",
    "tgt_dico, _tgt_emb = load_embedding(tgt_emb_file, tgt_lang)\n",
    "tgt_emb = torch.nn.Embedding(len(tgt_dico), emb_dim, sparse=True)\n",
    "tgt_emb.weight.data.copy_(_tgt_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (layers): Sequential(\n",
       "    (0): Dropout(p=0.1)\n",
       "    (1): Linear(in_features=300, out_features=2048, bias=True)\n",
       "    (2): LeakyReLU(negative_slope=0.2)\n",
       "    (3): Dropout(p=0.0)\n",
       "    (4): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "    (5): LeakyReLU(negative_slope=0.2)\n",
       "    (6): Dropout(p=0.0)\n",
       "    (7): Linear(in_features=2048, out_features=1, bias=True)\n",
       "    (8): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generator\n",
    "mapping = torch.nn.Linear(emb_dim, emb_dim, bias=False)\n",
    "mapping.weight.data.copy_(torch.diag(torch.ones(emb_dim)))\n",
    "\n",
    "# Discriminator\n",
    "class Discriminator(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.layers = [torch.nn.Dropout(dis_input_dropout)]\n",
    "        self.layers.append(torch.nn.Linear(emb_dim, dis_hid_dim))\n",
    "        self.layers.append(torch.nn.LeakyReLU(0.2))\n",
    "        self.layers.append(torch.nn.Dropout(dis_dropout))\n",
    "        self.layers.append(torch.nn.Linear(dis_hid_dim, dis_hid_dim))\n",
    "        self.layers.append(torch.nn.LeakyReLU(0.2))\n",
    "        self.layers.append(torch.nn.Dropout(dis_dropout))\n",
    "        self.layers.append(torch.nn.Linear(dis_hid_dim, 1))\n",
    "        self.layers.append(torch.nn.Sigmoid())\n",
    "        self.layers = torch.nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "            assert x.dim() == 2 and x.size(1) == emb_dim\n",
    "            return self.layers(x).view(-1)\n",
    "\n",
    "discriminator = Discriminator()\n",
    "        \n",
    "# Cuda\n",
    "src_emb.cuda()\n",
    "tgt_emb.cuda()\n",
    "mapping.cuda()\n",
    "discriminator.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "optimizer_g = optim.SGD(mapping.parameters(), lr)\n",
    "optimizer_d = optim.SGD(discriminator.parameters(), lr)\n",
    "lambda_ = lambda epoch: decay ** epoch\n",
    "scheduler_g = LambdaLR(optimizer_g, lr_lambda=[lambda_])\n",
    "scheduler_d = LambdaLR(optimizer_d, lr_lambda=[lambda_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===== ADVERSARIAL TRAINING =====\n",
      "\n",
      "start epoch 0\n",
      "iteration 0, loss 0.6615\n",
      "iteration 12000, loss 0.6392\n",
      "iteration 24000, loss 0.4954\n",
      "iteration 36000, loss 0.4622\n",
      "iteration 48000, loss 0.4953\n",
      "iteration 60000, loss 0.4206\n",
      "iteration 72000, loss 0.3937\n",
      "iteration 84000, loss 0.4451\n",
      "iteration 96000, loss 0.4282\n",
      "iteration 108000, loss 0.4642\n",
      "iteration 120000, loss 0.3816\n",
      "iteration 132000, loss 0.4126\n",
      "iteration 144000, loss 0.4100\n",
      "iteration 156000, loss 0.3983\n",
      "iteration 168000, loss 0.4119\n",
      "iteration 180000, loss 0.4112\n",
      "iteration 192000, loss 0.4239\n",
      "csls unsupervised metric score: -0.01340\n",
      "cross-lingual word similarity score average: 0.18449\n",
      "found 11977 pairs of words in the dictionary. 0 other pairs contained at least one unknown word (0 in lang1, 0 in lang2)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1524584710464/work/aten/src/THC/generic/THCStorage.cu:58",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-91c865f0b644>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;31m# word translation evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     word_translation_nn_result = get_word_translation_accuracy(src_lang, src_dico.word2id, mapping(src_emb.weight).data, \n\u001b[0;32m---> 77\u001b[0;31m                     tgt_lang, tgt_dico.word2id, tgt_emb.weight.data, 'nn')\n\u001b[0m\u001b[1;32m     78\u001b[0m     word_translation_csls_result = get_word_translation_accuracy(src_lang, src_dico.word2id, mapping(src_emb.weight).data, \n\u001b[1;32m     79\u001b[0m                     tgt_lang, tgt_dico.word2id, tgt_emb.weight.data, 'csls_knn_10')\n",
      "\u001b[0;32m~/UWT/evaluator.py\u001b[0m in \u001b[0;36mget_word_translation_accuracy\u001b[0;34m(lang1, word2id1, emb1, lang2, word2id2, emb2, method)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nn'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memb1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdico\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;31m# cross-domain similarity local scaling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch_1524584710464/work/aten/src/THC/generic/THCStorage.cu:58"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "from torch.nn import functional\n",
    "from evaluator import get_crosslingual_wordsim_scores\n",
    "from evaluator import get_word_translation_accuracy\n",
    "from evaluator import get_unsupervised_evaluation\n",
    "\n",
    "assert dis_most_frequent <= min(len(src_dico), len(tgt_dico))\n",
    "\n",
    "def get_xy(src_emb, tgt_emb, vol):\n",
    "    src_ids = torch.LongTensor(batch_size).random_(dis_most_frequent).cuda()\n",
    "    tgt_ids = torch.LongTensor(batch_size).random_(dis_most_frequent).cuda()\n",
    "    src_emb_ = src_emb(Variable(src_ids))\n",
    "    tgt_emb_ = tgt_emb(Variable(tgt_ids))\n",
    "    src_emb_ = mapping(Variable(src_emb_.data))\n",
    "    tgt_emb_ = Variable(tgt_emb_.data)\n",
    "    x = torch.cat([src_emb_, tgt_emb_], 0)\n",
    "    y = torch.FloatTensor(2 * batch_size).zero_()\n",
    "    y[:batch_size] = 1 - dis_smooth\n",
    "    y[batch_size:] = dis_smooth\n",
    "    y = Variable(y.cuda())\n",
    "    return x, y\n",
    "\n",
    "eval_list = []\n",
    "logger.info('===== ADVERSARIAL TRAINING =====')\n",
    "for epoch in range(n_epochs):\n",
    "    logger.info('\\nstart epoch %i' % epoch)\n",
    "    \n",
    "    for i_iter in range(0, epoch_size, batch_size):\n",
    "        \n",
    "        ## discriminiator\n",
    "        for i_dis in range(dis_steps):\n",
    "            discriminator.train()\n",
    "            x, y = get_xy(src_emb, tgt_emb, True)\n",
    "            preds = discriminator(Variable(x.data))\n",
    "            loss = functional.binary_cross_entropy(preds, y)\n",
    "            optimizer_d.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_d.step()\n",
    "            \n",
    "        if i_iter % 3000 == 0:\n",
    "            logger.info('iteration %s, loss %.4f' % (i_iter, loss.data.item()))\n",
    "            \n",
    "        ## generator\n",
    "        discriminator.eval()\n",
    "        x, y = get_xy(src_emb, tgt_emb, False)\n",
    "        preds = discriminator(x)\n",
    "        loss = functional.binary_cross_entropy(preds, 1 - y)\n",
    "        optimizer_g.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_g.step()\n",
    "        W = mapping.weight.data\n",
    "        W.copy_((1 + map_beta) * W - map_beta * W.mm(W.transpose(0, 1).mm(W)))\n",
    "        \n",
    "        if i_iter == 0:\n",
    "            optimizer_d.zero_grad()\n",
    "            scheduler_d.step()\n",
    "            optimizer_g.zero_grad()\n",
    "            scheduler_g.step()\n",
    "        \n",
    "    # unsupervised evaluation metric\n",
    "    unsupervised_score = get_unsupervised_evaluation(src_dico.word2id, mapping(src_emb.weight).data, tgt_dico.word2id, tgt_emb.weight.data, 10)\n",
    "    if len(eval_list)>0 and unsupervised_score < min(eval_list):\n",
    "        for g in optimizer_g.param_groups:\n",
    "            g['lr'] = g['lr']/2\n",
    "        for g in optimizer_d.param_groups:\n",
    "            g['lr'] = g['lr']/2\n",
    "    eval_list.append(unsupervised_score)\n",
    "    logger.info(\"csls unsupervised metric score: %.5f\" % unsupervised_score)\n",
    "    \n",
    "    # cross-lingual similarity evaluation\n",
    "    src_tgt_ws_score = get_crosslingual_wordsim_scores(src_lang, src_dico.word2id, mapping(src_emb.weight).data.cpu().numpy(), \n",
    "                                                        tgt_lang, tgt_dico.word2id, tgt_emb.weight.data.cpu().numpy())\n",
    "    logger.info(\"cross-lingual word similarity score average: %.5f\" % src_tgt_ws_score)\n",
    "    \n",
    "    # word translation evaluation\n",
    "    word_translation_nn_result = get_word_translation_accuracy(src_lang, src_dico.word2id, mapping(src_emb.weight).data, \n",
    "                    tgt_lang, tgt_dico.word2id, tgt_emb.weight.data, 'nn')\n",
    "    word_translation_csls_result = get_word_translation_accuracy(src_lang, src_dico.word2id, mapping(src_emb.weight).data, \n",
    "                    tgt_lang, tgt_dico.word2id, tgt_emb.weight.data, 'csls_knn_10')\n",
    "\n",
    "logger.info('End time: %s' % datetime.now().strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
